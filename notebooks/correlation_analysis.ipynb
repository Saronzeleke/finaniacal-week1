{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9885d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from task3_correlation_analysis import CorrelationAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40a605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Correlation Analysis between News Sentiment and Stock Returns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Task 3: Correlation Analysis between News Sentiment and Stock Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the news data\n",
    "print(\"Loading news data...\")\n",
    "news_df = pd.read_csv(r'C:\\Users\\admin\\finaniacal-week1\\data\\raw_analyst_ratings.csv')\n",
    "print(\"News data columns:\", news_df.columns.tolist())\n",
    "print(\"News data shape:\", news_df.shape)\n",
    "print(\"\\nFirst few rows of news data:\")\n",
    "display(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc58291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the stock data\n",
    "print(\"Loading stock data...\")\n",
    "stock_df = pd.read_csv(r'C:\\Users\\admin\\finaniacal-week1\\data\\NVDA.csv')\n",
    "print(\"Stock data columns:\", stock_df.columns.tolist())\n",
    "print(\"Stock data shape:\", stock_df.shape)\n",
    "print(\"\\nFirst few rows of stock data:\")\n",
    "display(stock_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fd37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded news data with 1407328 records\n",
      "Loaded stock data with 3774 records\n",
      "Error loading data: 'publication_date'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'publication_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\finaniacal-week1\\my_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'publication_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display basic information about the datasets\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNews Data Overview:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\finaniacal-week1\\notebooks\\../src\\task3_correlation_analysis.py:50\u001b[39m, in \u001b[36mCorrelationAnalyzer.load_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded stock data with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.stock_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Convert date columns to datetime\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_normalize_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     53\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError loading data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\finaniacal-week1\\notebooks\\../src\\task3_correlation_analysis.py:61\u001b[39m, in \u001b[36mCorrelationAnalyzer._normalize_dates\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03mNormalize date formats between news and stock data\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Convert news date to datetime and extract date only (remove time)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28mself\u001b[39m.news_data[\u001b[33m'\u001b[39m\u001b[33mpublication_date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnews_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpublication_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m).dt.normalize()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Convert stock date to datetime\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m.stock_data[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[38;5;28mself\u001b[39m.stock_data[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m]).dt.normalize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\finaniacal-week1\\my_env\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\finaniacal-week1\\my_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'publication_date'"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and cleaning\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Check for missing values in news data\n",
    "print(\"Missing values in news data:\")\n",
    "print(news_df.isnull().sum())\n",
    "\n",
    "# Check for missing values in stock data\n",
    "print(\"\\nMissing values in stock data:\")\n",
    "print(stock_df.isnull().sum())\n",
    "\n",
    "# Check date ranges\n",
    "print(f\"\\nNews date range: {news_df['date'].min()} to {news_df['date'].max()}\")\n",
    "print(f\"Stock date range: {stock_df['Date'].min()} to {stock_df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the correlation analyzer with correct column mappings\n",
    "analyzer = CorrelationAnalyzer(\n",
    "    news_data_path=r'C:\\Users\\admin\\finaniacal-week1\\data\\raw_analyst_ratings.csv',\n",
    "    stock_data_path=r'C:\\Users\\admin\\finaniacal-week1\\data\\NVDA.csv'\n",
    ")\n",
    "\n",
    "# Manually set the data with proper column names\n",
    "analyzer.news_data = news_df.rename(columns={'date': 'publication_date'})\n",
    "analyzer.stock_data = stock_df\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"News records: {len(analyzer.news_data)}\")\n",
    "print(f\"Stock records: {len(analyzer.stock_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11863914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize dates\n",
    "print(\"Normalizing dates...\")\n",
    "analyzer._normalize_dates()\n",
    "\n",
    "# Check the normalized data\n",
    "print(f\"News date range after normalization: {analyzer.news_data['publication_date'].min()} to {analyzer.news_data['publication_date'].max()}\")\n",
    "print(f\"Stock date range after normalization: {analyzer.stock_data['Date'].min()} to {analyzer.stock_data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02951b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Perform sentiment analysis on headlines\n",
    "print(\"Performing sentiment analysis...\")\n",
    "sentiment_data = analyzer.analyze_sentiment(text_column='headline')\n",
    "\n",
    "print(\"Sentiment analysis results:\")\n",
    "display(sentiment_data.head(10))\n",
    "\n",
    "print(f\"\\nSentiment statistics:\")\n",
    "print(f\"Average sentiment: {sentiment_data['avg_sentiment'].mean():.3f}\")\n",
    "print(f\"Sentiment std: {sentiment_data['avg_sentiment'].std():.3f}\")\n",
    "print(f\"Min sentiment: {sentiment_data['avg_sentiment'].min():.3f}\")\n",
    "print(f\"Max sentiment: {sentiment_data['avg_sentiment'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sentiment_data['avg_sentiment'], bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Average Daily Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Daily Average Sentiment')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sentiment_data['publication_date'], sentiment_data['avg_sentiment'], marker='o', linewidth=1, markersize=2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.title('Sentiment Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f39ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compute daily returns\n",
    "print(\"Computing daily returns...\")\n",
    "returns_data = analyzer.compute_daily_returns(price_column='Close')\n",
    "\n",
    "print(\"Returns data sample:\")\n",
    "display(returns_data.head(10))\n",
    "\n",
    "print(f\"\\nReturns statistics:\")\n",
    "print(f\"Average daily return: {returns_data['daily_return'].mean():.3f}%\")\n",
    "print(f\"Returns std: {returns_data['daily_return'].std():.3f}%\")\n",
    "print(f\"Min return: {returns_data['daily_return'].min():.3f}%\")\n",
    "print(f\"Max return: {returns_data['daily_return'].max():.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f13a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stock returns\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(returns_data['Date'], returns_data['daily_return'], color='green', linewidth=1)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Return (%)')\n",
    "plt.title('NVDA Daily Returns Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(returns_data['daily_return'], bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "plt.xlabel('Daily Return (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Daily Returns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5463369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Merge sentiment and returns data\n",
    "print(\"Merging sentiment and returns data...\")\n",
    "merged_data = analyzer.merge_sentiment_returns()\n",
    "\n",
    "print(\"Merged data sample:\")\n",
    "display(merged_data.head(10))\n",
    "\n",
    "print(f\"\\nMerged dataset statistics:\")\n",
    "print(f\"Total matching days: {len(merged_data)}\")\n",
    "print(f\"Date range: {merged_data['publication_date'].min()} to {merged_data['publication_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate correlation\n",
    "print(\"Calculating correlation coefficients...\")\n",
    "correlation_results = analyzer.calculate_correlation(merged_data)\n",
    "\n",
    "print(\"Correlation Results:\")\n",
    "for key, value in correlation_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8731118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create comprehensive visualizations\n",
    "print(\"Generating correlation visualizations...\")\n",
    "analyzer.visualize_correlation(merged_data, save_path='../results/task3_correlation_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Sentiment vs Returns by publisher\n",
    "if 'publisher' in analyzer.news_data.columns:\n",
    "    print(\"Analyzing sentiment by publisher...\")\n",
    "    \n",
    "    # Get top publishers by article count\n",
    "    top_publishers = analyzer.news_data['publisher'].value_counts().head(10).index.tolist()\n",
    "    \n",
    "    # Create publisher-specific analysis\n",
    "    publisher_correlations = {}\n",
    "    \n",
    "    for publisher in top_publishers:\n",
    "        publisher_news = analyzer.news_data[analyzer.news_data['publisher'] == publisher]\n",
    "        publisher_sentiment = publisher_news.groupby('publication_date')['sentiment'].mean().reset_index()\n",
    "        \n",
    "        # Merge with returns\n",
    "        publisher_merged = pd.merge(\n",
    "            publisher_sentiment,\n",
    "            analyzer.stock_data[['Date', 'Close']],\n",
    "            left_on='publication_date',\n",
    "            right_on='Date',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Calculate returns\n",
    "        publisher_merged['daily_return'] = publisher_merged['Close'].pct_change() * 100\n",
    "        publisher_merged = publisher_merged.dropna()\n",
    "        \n",
    "        if len(publisher_merged) > 5:  # Require minimum data points\n",
    "            corr, p_value = pearsonr(publisher_merged['sentiment'], publisher_merged['daily_return'])\n",
    "            publisher_correlations[publisher] = {\n",
    "                'correlation': corr,\n",
    "                'p_value': p_value,\n",
    "                'articles': len(publisher_news),\n",
    "                'days_with_data': len(publisher_merged)\n",
    "            }\n",
    "    \n",
    "    # Display publisher correlations\n",
    "    publisher_corr_df = pd.DataFrame(publisher_correlations).T.sort_values('correlation', ascending=False)\n",
    "    print(\"\\nPublisher-specific correlations:\")\n",
    "    display(publisher_corr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Compute Daily Returns and Prepare for Correlation\n",
    "print(\"Computing Daily Returns and Merging Data...\")\n",
    "\n",
    "# Compute daily returns\n",
    "returns_data = analyzer.compute_daily_returns()\n",
    "\n",
    "print(\"\\nReturns Statistics:\")\n",
    "print(f\"Average Daily Return: {returns_data['daily_return'].mean():.4f}%\")\n",
    "print(f\"Return Std Dev: {returns_data['daily_return'].std():.4f}%\")\n",
    "print(f\"Maximum Daily Return: {returns_data['daily_return'].max():.4f}%\")\n",
    "print(f\"Minimum Daily Return: {returns_data['daily_return'].min():.4f}%\")\n",
    "# Display returns data\n",
    "display(returns_data.head(10))\n",
    "\n",
    "# Plot returns distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(returns_data['daily_return'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Daily Returns')\n",
    "plt.xlabel('Daily Return (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(returns_data['Date'], returns_data['daily_return'], alpha=0.7)\n",
    "plt.title('Daily Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Daily Return (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Generate final report\n",
    "print(\"Generating comprehensive report...\")\n",
    "analyzer.generate_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataset for future use\n",
    "print(\"Saving results...\")\n",
    "merged_data.to_csv('../data/processed/sentiment_returns_merged.csv', index=False)\n",
    "print(\"Merged data saved to '../data/processed/sentiment_returns_merged.csv'\")\n",
    "\n",
    "# Save correlation results\n",
    "correlation_df = pd.DataFrame([analyzer.correlation_results])\n",
    "correlation_df.to_csv('../results/task3_correlation_results.csv', index=False)\n",
    "print(\"Correlation results saved to '../results/task3_correlation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed90b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analysis: Rolling correlation\n",
    "print(\"Calculating rolling correlation...\")\n",
    "\n",
    "# Sort by date\n",
    "merged_sorted = merged_data.sort_values('publication_date').reset_index(drop=True)\n",
    "\n",
    "# Calculate 30-day rolling correlation\n",
    "window_size = 30\n",
    "rolling_corr = []\n",
    "\n",
    "for i in range(len(merged_sorted) - window_size + 1):\n",
    "    window_data = merged_sorted.iloc[i:i + window_size]\n",
    "    if len(window_data) >= window_size:\n",
    "        corr, _ = pearsonr(window_data['avg_sentiment'], window_data['daily_return'])\n",
    "        rolling_corr.append(corr)\n",
    "    else:\n",
    "        rolling_corr.append(np.nan)\n",
    "\n",
    "# Add to dataframe\n",
    "merged_sorted['rolling_correlation'] = [np.nan] * (window_size - 1) + rolling_corr\n",
    "\n",
    "# Plot rolling correlation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_sorted['publication_date'], merged_sorted['rolling_correlation'], \n",
    "         linewidth=2, color='purple', alpha=0.8)\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(f'Rolling Correlation ({window_size}-day window)')\n",
    "plt.title('Rolling Correlation between News Sentiment and Stock Returns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average rolling correlation: {np.nanmean(rolling_corr):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and insights\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 3 COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä DATA OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ News articles analyzed: {len(analyzer.news_data):,}\")\n",
    "print(f\"   ‚Ä¢ Stock trading days: {len(analyzer.stock_data):,}\")\n",
    "print(f\"   ‚Ä¢ Matching days for analysis: {len(merged_data):,}\")\n",
    "\n",
    "print(f\"\\nüîç SENTIMENT ANALYSIS:\")\n",
    "print(f\"   ‚Ä¢ Average daily sentiment: {sentiment_data['avg_sentiment'].mean():.3f}\")\n",
    "print(f\"   ‚Ä¢ Sentiment volatility (std): {sentiment_data['avg_sentiment'].std():.3f}\")\n",
    "\n",
    "print(f\"\\nüìà STOCK PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ Average daily return: {returns_data['daily_return'].mean():.3f}%\")\n",
    "print(f\"   ‚Ä¢ Return volatility (std): {returns_data['daily_return'].std():.3f}%\")\n",
    "\n",
    "print(f\"\\nüìä CORRELATION RESULTS:\")\n",
    "corr_strength = abs(analyzer.correlation_results['daily_return_correlation'])\n",
    "if corr_strength < 0.1:\n",
    "    strength = \"negligible\"\n",
    "elif corr_strength < 0.3:\n",
    "    strength = \"weak\"\n",
    "elif corr_strength < 0.5:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"strong\"\n",
    "\n",
    "print(f\"   ‚Ä¢ Correlation strength: {strength} ({analyzer.correlation_results['daily_return_correlation']:.4f})\")\n",
    "print(f\"   ‚Ä¢ Statistical significance: {'YES' if analyzer.correlation_results['daily_return_p_value'] < 0.05 else 'NO'}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "if analyzer.correlation_results['daily_return_p_value'] < 0.05:\n",
    "    if analyzer.correlation_results['daily_return_correlation'] > 0:\n",
    "        print(\"   ‚Ä¢ Positive news sentiment tends to correlate with higher stock returns\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Positive news sentiment tends to correlate with lower stock returns\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No statistically significant relationship found between sentiment and returns\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "print(\"   ‚Ä¢ Consider incorporating sentiment analysis in trading strategies\")\n",
    "print(\"   ‚Ä¢ Monitor specific publisher sentiment for more targeted insights\")\n",
    "print(\"   ‚Ä¢ Expand analysis to include other technical indicators\")\n",
    "print(\"   ‚Ä¢ Consider lagged effects for predictive modeling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Task 3: Correlation Analysis - COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd00a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Comprehensive Visualization\n",
    "print(\"Generating Comprehensive Visualizations...\")\n",
    "\n",
    "# Generate all visualizations\n",
    "analyzer.visualize_correlation(merged_data, save_path='../results/task3_correlation_analysis.png')\n",
    "\n",
    "# Additional custom visualizations\n",
    "print(\"\\nGenerating Additional Custom Visualizations...\")\n",
    "# 1. Rolling correlation over time\n",
    "window_size = 30  # 30-day rolling window\n",
    "merged_data_sorted = merged_data.sort_values('publication_date').copy()\n",
    "merged_data_sorted['rolling_corr'] = merged_data_sorted['avg_sentiment'].rolling(window=window_size).corr(merged_data_sorted['daily_return'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90783d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(merged_data_sorted['publication_date'], merged_data_sorted['rolling_corr'], \n",
    "         color='purple', linewidth=2)\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "plt.title(f'{window_size}-Day Rolling Correlation Between Sentiment and Returns')\n",
    "plt.ylabel('Rolling Correlation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(merged_data_sorted['publication_date'], merged_data_sorted['avg_sentiment'], \n",
    "         label='Sentiment', alpha=0.7)\n",
    "plt.plot(merged_data_sorted['publication_date'], merged_data_sorted['daily_return'], \n",
    "         label='Returns', alpha=0.7)\n",
    "plt.title('Sentiment and Returns Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Sentiment vs Returns by sentiment categories\n",
    "merged_data['sentiment_category'] = pd.cut(merged_data['avg_sentiment'], \n",
    "                                          bins=[-1, -0.1, 0.1, 1], \n",
    "                                          labels=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=merged_data, x='sentiment_category', y='daily_return')\n",
    "plt.title('Daily Returns by Sentiment Category')\n",
    "plt.xlabel('Sentiment Category')\n",
    "plt.ylabel('Daily Return (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Scatter plot with regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(data=merged_data, x='avg_sentiment', y='daily_return', \n",
    "            scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "plt.title(f'Sentiment vs Daily Returns (Correlation: {correlation_results[\"daily_return_correlation\"]:.3f})')\n",
    "plt.xlabel('Average Daily Sentiment')\n",
    "plt.ylabel('Daily Return (%)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Advanced Analysis - Lagged Effects\n",
    "print(\"Analyzing Lagged Effects...\")\n",
    "\n",
    "# Create lagged variables for analysis\n",
    "lags = range(0, 6)  \n",
    "lag_correlations = []\n",
    "\n",
    "for lag in lags:\n",
    "    if lag == 0:\n",
    "        # Same day correlation\n",
    "        corr, p_val = pearsonr(merged_data['avg_sentiment'], merged_data['daily_return'])\n",
    "    else:\n",
    "        # Create lagged returns (sentiment today vs returns in future)\n",
    "        temp_data = merged_data.copy()\n",
    "        temp_data[f'return_lag_{lag}'] = temp_data['daily_return'].shift(-lag)\n",
    "        temp_data_lagged = temp_data.dropna()\n",
    "        \n",
    "        if len(temp_data_lagged) > 0:\n",
    "            corr, p_val = pearsonr(temp_data_lagged['avg_sentiment'], \n",
    "                                 temp_data_lagged[f'return_lag_{lag}'])\n",
    "        else:\n",
    "            corr, p_val = (np.nan, np.nan)\n",
    "    \n",
    "    lag_correlations.append({\n",
    "        'lag_days': lag,\n",
    "        'correlation': corr,\n",
    "        'p_value': p_val,\n",
    "        'significant': p_val < 0.05 if not np.isnan(p_val) else False\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "lag_results = pd.DataFrame(lag_correlations)\n",
    "\n",
    "print(\"Lagged Correlation Analysis:\")\n",
    "display(lag_results)\n",
    "\n",
    "# Plot lagged correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(lag_results['lag_days'], lag_results['correlation'], \n",
    "               color=['red' if sig else 'blue' for sig in lag_results['significant']],\n",
    "               alpha=0.7)\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.title('Correlation Between Sentiment and Future Returns (Lagged Analysis)')\n",
    "plt.xlabel('Lag (Days)')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(lags)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, corr in zip(bars, lag_results['correlation']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01 * (1 if bar.get_height() > 0 else -1),\n",
    "             f'{corr:.3f}', ha='center', va='bottom' if bar.get_height() > 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Section 11: Save Results and Generate Final Report\n",
    "print(\"Saving Results and Generating Final Report...\")\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_data.to_csv('../data/processed/sentiment_returns_merged.csv', index=False)\n",
    "print(\"Saved merged dataset to: ../data/processed/sentiment_returns_merged.csv\")\n",
    "\n",
    "# Save correlation results\n",
    "correlation_df = pd.DataFrame([correlation_results])\n",
    "correlation_df.to_csv('../results/task3_correlation_results.csv', index=False)\n",
    "print(\"Saved correlation results to: ../results/task3_correlation_results.csv\")\n",
    "\n",
    "# Save lag analysis results\n",
    "lag_results.to_csv('../results/task3_lag_analysis.csv', index=False)\n",
    "print(\"Saved lag analysis results to: ../results/task3_lag_analysis.csv\")\n",
    "\n",
    "# Generate final comprehensive report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 3: FINAL CORRELATION ANALYSIS REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDATA OVERVIEW:\")\n",
    "print(f\"- News articles analyzed: {len(analyzer.news_data)}\")\n",
    "print(f\"- Trading days analyzed: {len(analyzer.stock_data)}\")\n",
    "print(f\"- Matching days with both sentiment and returns: {len(merged_data)}\")\n",
    "print(f\"- Analysis period: {merged_data['publication_date'].min().strftime('%Y-%m-%d')} to {merged_data['publication_date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(f\"1. Primary Correlation (Same Day): {correlation_results['daily_return_correlation']:.4f}\")\n",
    "print(f\"   Statistical Significance: {'Yes' if correlation_results['daily_return_p_value'] < 0.05 else 'No'}\")\n",
    "\n",
    "if not np.isnan(correlation_results['lagged_correlation']):\n",
    "    print(f\"2. Predictive Correlation (Next Day): {correlation_results['lagged_correlation']:.4f}\")\n",
    "    print(f\"   Statistical Significance: {'Yes' if correlation_results['lagged_p_value'] < 0.05 else 'No'}\")\n",
    "\n",
    "print(f\"\\nINTERPRETATION:\")\n",
    "if abs(correlation_results['daily_return_correlation']) > 0.3 and correlation_results['daily_return_p_value'] < 0.05:\n",
    "    print(\"‚úÖ STRONG EVIDENCE of relationship between news sentiment and stock returns\")\n",
    "elif abs(correlation_results['daily_return_correlation']) > 0.1 and correlation_results['daily_return_p_value'] < 0.05:\n",
    "    print(\"‚ö†Ô∏è MODERATE EVIDENCE of relationship between news sentiment and stock returns\")\n",
    "else:\n",
    "    print(\"‚ùï WEAK or NO EVIDENCE of relationship between news sentiment and stock returns\")\n",
    "\n",
    "print(f\"\\nRECOMMENDATIONS:\")\n",
    "if correlation_results['daily_return_p_value'] < 0.05:\n",
    "    if correlation_results['daily_return_correlation'] > 0:\n",
    "        print(\"- Positive news sentiment tends to correlate with positive stock returns\")\n",
    "        print(\"- Consider incorporating sentiment analysis in trading strategies\")\n",
    "    else:\n",
    "        print(\"- Negative news sentiment tends to correlate with negative stock returns\")\n",
    "        print(\"- Sentiment could be used as a contrarian indicator\")\n",
    "else:\n",
    "    print(\"- No statistically significant relationship found\")\n",
    "    print(\"- News sentiment may not be a reliable indicator for this dataset/time period\")\n",
    "\n",
    "print(f\"\\nFILES GENERATED:\")\n",
    "print(\"1. ../data/processed/sentiment_returns_merged.csv - Merged dataset\")\n",
    "print(\"2. ../results/task3_correlation_results.csv - Correlation coefficients\")\n",
    "print(\"3. ../results/task3_lag_analysis.csv - Lagged analysis results\")\n",
    "print(\"4. ../results/task3_correlation_analysis.png - Comprehensive visualizations\")\n",
    "\n",
    "print(f\"\\nTask 3 Correlation Analysis Completed Successfully! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
